import os
import re
import csv
import json
from collections import defaultdict
from selenium import webdriver
from selenium.webdriver.common.by import By
from tqdm import tqdm

# Ã‡Ä±xÄ±ÅŸ fayllarÄ± Ã¼Ã§Ã¼n É™sas qovluq
OUTPUT_FOLDER = "output_files"
os.makedirs(OUTPUT_FOLDER, exist_ok=True)  # Qovluq mÃ¶vcud deyilsÉ™, yaradÄ±lÄ±r

# Ã‡Ä±xÄ±ÅŸ fayllarÄ±nÄ±n yollarÄ±
URL_STATUS_REPORT_FILE = os.path.join(OUTPUT_FOLDER, "url_status_report.txt")
MALWARE_CANDIDATES_FILE = os.path.join(OUTPUT_FOLDER, "malware_candidates.csv")
ALERT_JSON_FILE = os.path.join(OUTPUT_FOLDER, "alert.json")
SUMMARY_REPORT_FILE = os.path.join(OUTPUT_FOLDER, "summary_report.json")

# 1. Access log-dan URL-lÉ™ri vÉ™ status kodlarÄ±nÄ± Ã§Ä±xarmaq
def parse_access_log(log_file):
    url_status = []
    with open(log_file, 'r') as file:
        for line in tqdm(file, desc="Access log mÉ™lumatlarÄ± analiz edilir"):
            match = re.search(r'(\S+) \S+ \S+ \[(.*?)\] "(.*?)" (\d{3})', line)
            if match:
                url = match.group(3).split()[1]  # URL
                status_code = match.group(4)    # Status kodu
                url_status.append((url, status_code))
    return url_status

# 2. 404 status kodu ilÉ™ URL-lÉ™ri mÃ¼É™yyÉ™n etmÉ™k
def count_404_urls(url_status):
    count = defaultdict(int)
    for url, status in tqdm(url_status, desc="404 URL-lÉ™ri hesablanÄ±r"):
        if status == '404':
            count[url] += 1
    return count

# 3. URL-lÉ™ri status kodlarÄ± ilÉ™ fayla yazmaq
def write_url_status_report(url_status, output_file):
    with open(output_file, 'w') as file:
        for url, status in tqdm(url_status, desc="URL statuslarÄ± fayla yazÄ±lÄ±r"):
            file.write(f"{url} {status}\n")
    print(f"â¡ URL statuslarÄ± yazÄ±ldÄ±: {output_file}\n")

# 4. 404 URL-lÉ™ri CSV faylÄ±nda yazmaq
def write_malware_candidates(counts, output_file):
    with open(output_file, 'w', newline='') as csvfile:
        fieldnames = ['URL', '404_count']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        for url, count in tqdm(counts.items(), desc="404 URL-lÉ™ri CSV-yÉ™ yazÄ±lÄ±r"):
            writer.writerow({'URL': url, '404_count': count})
    print(f"â¡ 404 URL-lÉ™ri yazÄ±ldÄ±: {output_file}\n")

# 5. Veb scraping (Selenium ilÉ™)
def scrape_blacklist(url):
    driver = webdriver.Chrome()
    driver.get(url)
    blacklist = []
    try:
        elements = driver.find_elements(By.XPATH, "//li")
        for element in tqdm(elements, desc="Qara siyahÄ±dan mÉ™lumat yÃ¼klÉ™nir"):
            blacklist.append(element.text)
    finally:
        driver.quit()
    print(f"â¡ Qara siyahÄ± uÄŸurla analiz edildi: {url}\n")
    return blacklist

# 6. URL-lÉ™ri qara siyahÄ± ilÉ™ mÃ¼qayisÉ™ etmÉ™k
def find_matching_urls(url_status, blacklist):
    matches = []
    for url, status in tqdm(url_status, desc="Qara siyahÄ± ilÉ™ URL-lÉ™r uyÄŸunlaÅŸdÄ±rÄ±lÄ±r"):
        domain = re.sub(r'https?://(www\.)?', '', url).split('/')[0]
        if domain in blacklist:
            matches.append((url, status))
    return matches

# 7. JSON faylÄ±nda uyÄŸun URL-lÉ™ri yazmaq
def write_alert_json(matches, output_file):
    alerts = [{'url': url, 'status': status} for url, status in matches]
    with open(output_file, 'w') as json_file:
        json.dump(alerts, json_file, indent=4)
    print(f"â¡ UyÄŸun URL-lÉ™r JSON formatÄ±nda yazÄ±ldÄ±: {output_file}\n")

# 8. XÃ¼lasÉ™ hesabatÄ± yaratmaq
def write_summary_report(url_status, counts, output_file):
    summary = {
        'total_urls': len(url_status),
        'total_404': sum(counts.values()),
        'unique_404_urls': len(counts)
    }
    with open(output_file, 'w') as json_file:
        json.dump(summary, json_file, indent=4)
    print(f"â¡ XÃ¼lasÉ™ hesabatÄ± yaradÄ±ldÄ±: {output_file}\n")

# Æsas funksiya
def main():
    log_file = 'access_log.txt'
    blacklist_url = 'http://127.0.0.1:8000'

    print("ğŸ” Analiz prosesi baÅŸlanÄ±r...\n")

    # 1. Access log-dan URL-lÉ™ri vÉ™ status kodlarÄ±nÄ± Ã§Ä±xarÄ±n
    url_status = parse_access_log(log_file)

    # 2. 404 status kodu ilÉ™ URL-lÉ™ri mÃ¼É™yyÉ™n edin
    counts = count_404_urls(url_status)

    # 3. URL-lÉ™rin siyahÄ±sÄ±nÄ± status kodlarÄ± ilÉ™ yazÄ±n
    write_url_status_report(url_status, URL_STATUS_REPORT_FILE)

    # 4. 404 URL-lÉ™ri CSV faylÄ±nda yazÄ±n
    write_malware_candidates(counts, MALWARE_CANDIDATES_FILE)

    # 5. Veb scraping (Selenium ilÉ™)
    blacklist = scrape_blacklist(blacklist_url)

    # 6. URL-lÉ™ri qara siyahÄ± ilÉ™ mÃ¼qayisÉ™ edin
    matches = find_matching_urls(url_status, blacklist)

    # 7. UyÄŸun URL-lÉ™ri JSON faylÄ±nda yazÄ±n
    write_alert_json(matches, ALERT_JSON_FILE)

    # 8. XÃ¼lasÉ™ hesabatÄ± yaradÄ±n
    write_summary_report(url_status, counts, SUMMARY_REPORT_FILE)

    print("âœ… Analiz prosesi tamamlandÄ±!")

if __name__ == "__main__":
    main()
